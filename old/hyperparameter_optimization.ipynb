{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import imagesize\n",
    "from torchvision import datasets, models, transforms\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from PIL.ImageFile import ImageFile\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import optuna\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#vars\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#batch_size = 128\n",
    "n_epochs = 15\n",
    "num_trials = 70\n",
    "validation_split = .25\n",
    "images_for_train = 30000\n",
    "print_every = 100\n",
    "\n",
    "name_training = \"efficient3_hyperparameters_wi\"\n",
    "os.mkdir(name_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "#data_path = \"D:\\Studium\\OneDrive - Hochschule Karlsruhe\\own-pictures\"\n",
    "\n",
    "image = []\n",
    "labels = []\n",
    "width = []\n",
    "height = []\n",
    "\n",
    "for file in os.listdir(data_path):\n",
    "    if file == 'glas':\n",
    "        for c in os.listdir(os.path.join(data_path, file)):\n",
    "            image.append(c)\n",
    "            labels.append('glas')\n",
    "            w, h = imagesize.get(data_path + \"/\" + file + \"/\" + c)\n",
    "            width.append(w), height.append(h)\n",
    "    if file == 'organic':\n",
    "        for c in os.listdir(os.path.join(data_path, file)):\n",
    "            image.append(c)\n",
    "            labels.append('organic')\n",
    "            w, h = imagesize.get(data_path + \"/\" + file + \"/\" + c)\n",
    "            width.append(w), height.append(h)\n",
    "    if file == 'paper':\n",
    "        for c in os.listdir(os.path.join(data_path, file)):\n",
    "            image.append(c)\n",
    "            labels.append('paper')\n",
    "            w, h = imagesize.get(data_path + \"/\" + file + \"/\" + c)\n",
    "            width.append(w), height.append(h)\n",
    "    if file == 'restmuell':\n",
    "        for c in os.listdir(os.path.join(data_path, file)):\n",
    "            image.append(c)\n",
    "            labels.append('restmuell')\n",
    "            w, h = imagesize.get(data_path + \"/\" + file + \"/\" + c)\n",
    "            width.append(w), height.append(h)\n",
    "    if file == 'wertstoff':\n",
    "        for c in os.listdir(os.path.join(data_path, file)):\n",
    "            image.append(c)\n",
    "            labels.append('wertstoff')\n",
    "            w, h = imagesize.get(data_path + \"/\" + file + \"/\" + c)\n",
    "            width.append(w), height.append(h)\n",
    "\n",
    "data = {'Images': image, 'labels': labels, 'width': width, 'height': height}\n",
    "data = pd.DataFrame(data)\n",
    "lb = LabelEncoder()\n",
    "data['encoded_labels'] = lb.fit_transform(data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data = data.head(images_for_train)\n",
    "tr, val = train_test_split(data, stratify=data.labels, test_size=validation_split)\n",
    "tr.reset_index(drop=True)\n",
    "val.reset_index(drop=True)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "waste_types_df = tr[['encoded_labels', 'labels']].drop_duplicates().sort_values(by='encoded_labels').reset_index(drop=True)\n",
    "garbage_types = {}\n",
    "for i in range(0, len(waste_types_df)):\n",
    "    garbage_types[i] = waste_types_df.iloc[i].labels\n",
    "print(garbage_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform_tr = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     #transforms.RandomRotation(10),\n",
    "     transforms.Resize((256,256), interpolation= transforms.InterpolationMode.BICUBIC),\n",
    "     #transforms.CenterCrop(290),\n",
    "     #transforms.Resize((300,300), interpolation= transforms.InterpolationMode.BICUBIC),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "transform_val = transforms.Compose(\n",
    "    [transforms.Resize((256,256), interpolation= transforms.InterpolationMode.BICUBIC),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GarbageDataset(Dataset):\n",
    "    def __init__(self, img_data, img_path, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        self.img_data = img_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.join(self.img_path, self.img_data.iloc[index].labels, self.img_data.iloc[index].Images)\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        #image = image.resize((300, 300))\n",
    "        label = torch.tensor(self.img_data.iloc[index].encoded_labels)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = GarbageDataset(tr, data_path, transform_tr)\n",
    "test_dataset = GarbageDataset(val, data_path, transform_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def img_display(img):\n",
    "    MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
    "    STD = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "    img = img * STD[:, None, None] + MEAN[:, None, None]\n",
    "    #img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    return npimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    _, pred = torch.max(out, dim=1)\n",
    "    return torch.sum(pred == labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    if model_name == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, len(waste_types_df))\n",
    "    elif model_name == \"alexnet\":\n",
    "        model = models.alexnet(pretrained=True)\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Linear(in_features, len(waste_types_df))\n",
    "    elif model_name == \"vgg16\":\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        in_features = model.classifier[0].in_features\n",
    "        model.classifier = nn.Linear(in_features, len(waste_types_df))\n",
    "    elif model_name == \"resnet50\":\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, len(waste_types_df))\n",
    "    elif model_name == \"efficientnet_b3\":\n",
    "        model = EfficientNet.from_pretrained(\"efficientnet-b3\",num_classes=len(waste_types_df))\n",
    "    elif model_name == \"resnet34\":\n",
    "        model = models.resnet34(pretrained=True)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, len(waste_types_df))\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters we want optimize\n",
    "    params = {\n",
    "        #\"model_name\": trial.suggest_categorical('model_name',[\"resnet50\", \"alexnet\", \"vgg16\"]),\n",
    "        \"lr\": trial.suggest_loguniform('lr', 1e-4, 1e-2),\n",
    "        \"optimizer_name\": trial.suggest_categorical('optimizer_name', [\"SGD\", \"Adam\", \"Adagrad\", \"RMSprop\"]),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "    }\n",
    "    #For SGD-Optimizer add momentum parameter\n",
    "\n",
    "    # Get pretrained model\n",
    "    model = get_model(\"efficientnet_b3\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Configure optimizer\n",
    "    optimizer = getattr(torch.optim, params[\"optimizer_name\"])(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    if params[\"optimizer_name\"] == \"SGD\":\n",
    "        params[\"momentum\"] = trial.suggest_loguniform('momentum', 0.85, 0.95)\n",
    "        # Configure optimizer again for SGD\n",
    "        optimizer = getattr(torch.optim, params[\"optimizer_name\"])(model.parameters(), lr=params[\"lr\"],\n",
    "                                                                   momentum=params[\"momentum\"])\n",
    "\n",
    "    # Train a model\n",
    "    best_model, best_loss = train_model(trial, model, criterion, optimizer, num_epochs=n_epochs,\n",
    "                                        b_size=params[\"batch_size\"])\n",
    "\n",
    "    # Return accuracy (Objective Value) of the current trial\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(trial, net, criterion, optimizer, num_epochs, b_size, step_size=7, gamma=0.1):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=b_size, shuffle=True, num_workers=7, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=b_size, shuffle=True, num_workers=7, pin_memory=True)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    valid_loss_min = np.Inf\n",
    "    total_step = len(train_dataloader)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(f'Epoch {epoch}\\n')\n",
    "        for batch_idx, (data_, target_, _) in enumerate(train_dataloader):\n",
    "            target_ = target_.type(torch.LongTensor)\n",
    "            data_, target_ = data_.to(device, non_blocking=True), target_.to(device, non_blocking=True)\n",
    "            # zero the parameter gradients\n",
    "            for param in net.parameters():\n",
    "                param.grad = None\n",
    "            #optimizer.zero_grad()\n",
    "            outputs = net(data_)\n",
    "            loss = criterion(outputs, target_)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            correct += torch.sum(pred == target_).item()\n",
    "            total += target_.size(0)\n",
    "            #if batch_idx % print_every == 0:\n",
    "                #print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, batch_idx, total_step, loss.item()))\n",
    "        train_acc.append(100 * correct / total)\n",
    "        train_loss.append(running_loss / total_step)\n",
    "        print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n",
    "        batch_loss = 0\n",
    "        total_t = 0\n",
    "        correct_t = 0\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for data_t, target_t, _ in (test_dataloader):\n",
    "                target_t = target_t.type(torch.LongTensor)\n",
    "                data_t, target_t = data_t.to(device), target_t.to(device)  # on GPU\n",
    "                outputs_t = net(data_t)\n",
    "                loss_t = criterion(outputs_t, target_t)\n",
    "                batch_loss += loss_t.item()\n",
    "                _, pred_t = torch.max(outputs_t, dim=1)\n",
    "                correct_t += torch.sum(pred_t == target_t).item()\n",
    "                total_t += target_t.size(0)\n",
    "            val_acc.append(100 * correct_t / total_t)\n",
    "            val_loss.append(batch_loss / len(test_dataloader))\n",
    "            network_learned = batch_loss < valid_loss_min\n",
    "            print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f}\\n')\n",
    "            # Saving the best weight\n",
    "            if network_learned:\n",
    "                valid_loss_min = batch_loss\n",
    "                torch.save(net.state_dict(), name_training + '/resnet.pt')\n",
    "                best_model = net\n",
    "                best_acc = (100 * correct_t / total_t)\n",
    "                best_loss = np.mean(val_loss)\n",
    "                print('Detected network improvement, saving current model')\n",
    "\n",
    "        net.train()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        trial.report(best_loss, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(\n",
    "    sampler=sampler,\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=2, n_warmup_steps=5, interval_steps=3\n",
    "    ),\n",
    "    direction='minimize')\n",
    "study.optimize(func=objective, n_trials=num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trial = study.best_trial\n",
    "\n",
    "print('Loss: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "text_file = open(name_training + \"/best_hyperparameters.txt\", \"w\")\n",
    "n = text_file.write(f'Loss: {trial.value}\\nBest hyperparams: {trial.params}')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "#fig.show()\n",
    "plt.savefig(name_training + '/param_importances.png', format=\"png\",bbox_inches='tight')\n",
    "plt.savefig(name_training + '/param_importances.pdf', format='pdf',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "#fig.show()\n",
    "plt.savefig(name_training + '/optim_history.png', format=\"png\",bbox_inches='tight')\n",
    "plt.savefig(name_training + '/optim_history.pdf', format='pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = optuna.visualization.matplotlib.plot_intermediate_values(study)\n",
    "#fig.show()\n",
    "plt.savefig(name_training + '/intermediate_values.png', format='png',bbox_inches='tight')\n",
    "plt.savefig(name_training + '/intermediate_values.pdf', format='pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = optuna.visualization.matplotlib.plot_slice(study)\n",
    "#fig.show()\n",
    "plt.savefig(name_training + '/plot_slice.png', format='png',bbox_inches='tight')\n",
    "plt.savefig(name_training + '/plot_slice.pdf', format='pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = optuna.visualization.matplotlib.plot_contour(study)\n",
    "#fig.show()\n",
    "plt.savefig(name_training + '/plot_contour.png', format='png',bbox_inches='tight')\n",
    "plt.savefig(name_training + '/plot_contour.pdf', format='pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}